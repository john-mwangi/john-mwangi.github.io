[{"content":"THE PROBLEM: You have two tables (such as below) and you need to match the Addresses in one to those in the adjacent table. How do you approach this challenge?\nIN SUMMARY: Use string distances to determine which is the most similar in your adjescent table. One such string distance is damerau_levenshtein which calculates this distance by determining the number of insertions, delections, transpositions, and substituions needed to change from one text to another.\nBecause these operations can be exponentially expensive, we\u0026rsquo;ll also examine how to apply multiprocessing - and by extension multithreading - to quicken the process.\nimport pandas as pd data1 = pd.read_csv(\u0026#34;./strings/task2_data1.csv\u0026#34;).head(10) data2 = pd.read_csv(\u0026#34;./strings/task2_data2.csv\u0026#34;).head(10) Fuzzy Join Matching the columns of two dataframes that both contain text data, using a string distance. The operation below returns a dictionary containing the two text columns as keys and string distances as the values.\nfrom textdistance import damerau_levenshtein damerau_levenshtein(s1=str(data1[\u0026#34;Address\u0026#34;][0]), s2=str(data2[\u0026#34;Address\u0026#34;][0])) dl_dist = {} for s1 in data1[\u0026#34;Address\u0026#34;]: for s2 in data2[\u0026#34;Address\u0026#34;]: temp_dist = damerau_levenshtein(s1, s2) dl_dist[f\u0026#34;{s1}_and_ {s2}\u0026#34;] = temp_dist list(dl_dist.items())[:10] [('Ulmenstr. 8 _and_ Alfons-Müller-Platz ', 16),\r('Ulmenstr. 8 _and_ Hauptstr. 10B', 8),\r('Ulmenstr. 8 _and_ Edisonstr. 36', 7),\r('Ulmenstr. 8 _and_ Kutterstr. 3 / 26386 Wilhelmshaven-Rüstersiel', 38),\r('Ulmenstr. 8 _and_ Am neuen Markt 8', 11),\r('Ulmenstr. 8 _and_ Rübenkamp 226', 10),\r('Ulmenstr. 8 _and_ Geisenheimer Str. 10', 14),\r('Ulmenstr. 8 _and_ Rathausplatz 1', 12),\r('Ulmenstr. 8 _and_ Weilimdorfer Str. 74 2', 15),\r('Ulmenstr. 8 _and_ Werkstr. 1', 6)]\r Now we begin the process of tidying up the dictionary. We ideally want to a table containing the two text columns and the distances between their entries.\ndl_dist_df_raw = pd.DataFrame.from_dict(dl_dist, orient=\u0026#34;index\u0026#34;).reset_index() dl_dist_df_raw.columns = [\u0026#34;addresses\u0026#34;, \u0026#34;dist\u0026#34;] dl_dist_df_proc = dl_dist_df_raw[\u0026#34;addresses\u0026#34;].str.split(pat=\u0026#34;_and_\u0026#34;, expand=True) dl_dist_df_proc.columns = [\u0026#34;address_1\u0026#34;, \u0026#34;address_2\u0026#34;] dl_dist_df_proc[\u0026#34;dist\u0026#34;] = dl_dist_df_raw[\u0026#34;dist\u0026#34;] dl_dist_df_proc \r\r\raddress_1\raddress_2\rdist\r\r\r\r\r0\rUlmenstr. 8\rAlfons-Müller-Platz\r16\r\r\r1\rUlmenstr. 8\rHauptstr. 10B\r8\r\r\r2\rUlmenstr. 8\rEdisonstr. 36\r7\r\r\r3\rUlmenstr. 8\rKutterstr. 3 / 26386 Wilhelmshaven-Rüstersiel\r38\r\r\r4\rUlmenstr. 8\rAm neuen Markt 8\r11\r\r\r...\r...\r...\r...\r\r\r95\rThomas-Schwarz-Str. 26\rRübenkamp 226\r19\r\r\r96\rThomas-Schwarz-Str. 26\rGeisenheimer Str. 10\r16\r\r\r97\rThomas-Schwarz-Str. 26\rRathausplatz 1\r18\r\r\r98\rThomas-Schwarz-Str. 26\rWeilimdorfer Str. 74 2\r18\r\r\r99\rThomas-Schwarz-Str. 26\rWerkstr. 1\r17\r\r\r\rFinally we select only the highest ranking entries. This are the most similar matching values.\ndl_dist_fin = dl_dist_df_proc.groupby(\u0026#34;address_1\u0026#34;, group_keys=False) \\ .apply(lambda df: df.sort_values(\u0026#34;dist\u0026#34;, ascending=True) \\ .head(1)) \\ .reset_index(drop=True) dl_dist_fin \r\raddress_1\raddress_2\rdist\r\r\r\r\r0\rAm Delf 31\rWerkstr. 1\r9\r\r\r1\rIm Kressgraben 18\rAm neuen Markt 8\r12\r\r\r2\rLeopold-Hoesch-Str. 4\rEdisonstr. 36\r15\r\r\r3\rMühlweg 12\rWerkstr. 1\r9\r\r\r4\rPaditzer Str. 33\rEdisonstr. 36\r9\r\r\r5\rSteingartenweg 12\rWerkstr. 1\r13\r\r\r6\rThomas-Schwarz-Str. 26\rEdisonstr. 36\r16\r\r\r7\rUlmenstr. 8\rWerkstr. 1\r6\r\r\r8\rWiesenstr. 11\rWerkstr. 1\r5\r\r\r9\rZaisentalstr. 70/1\rHauptstr. 10B\r10\r\r\r\rLast task is a simple join between this table and the original tables and dropping any unnecessary columns.\ndata_both = pd.merge(left=data1, right=dl_dist_fin, how=\u0026#34;right\u0026#34;, left_on=\u0026#34;Address\u0026#34;, right_on=\u0026#34;address_1\u0026#34;) data_both \r\rCompany.Name\rAddress\rCity\rPostcode\raddress_1\raddress_2\rdist\r\r\r\r\r0\rMario Tsiknas\rAm Delf 31\rBad Zwischenahn\r26160\rAm Delf 31\rWerkstr. 1\r9\r\r\r1\rAndre Hanisch\rIm Kressgraben 18\rUntereisesheim\r74257\rIm Kressgraben 18\rAm neuen Markt 8\r12\r\r\r2\rMatthias Essers GmbH Elektrote\rLeopold-Hoesch-Str. 4\rGeilenkirchen\r52511\rLeopold-Hoesch-Str. 4\rEdisonstr. 36\r15\r\r\r3\rGerold Fuchs\rMühlweg 12\rDietingen\r78661\rMühlweg 12\rWerkstr. 1\r9\r\r\r4\rZirpel \u0026amp; Pautzsch Ingenieur Pa\rPaditzer Str. 33\rAltenburg\r4600\rPaditzer Str. 33\rEdisonstr. 36\r9\r\r\r5\rEberhard Zessin\rSteingartenweg 12\rHeidelberg\r69118\rSteingartenweg 12\rWerkstr. 1\r13\r\r\r6\rPaul Strigl\rThomas-Schwarz-Str. 26\rDachau\r85221\rThomas-Schwarz-Str. 26\rEdisonstr. 36\r16\r\r\r7\rCarsten Helm\rUlmenstr. 8\rWismar\r23966\rUlmenstr. 8\rWerkstr. 1\r6\r\r\r8\rWolfgang Jäger\rWiesenstr. 11\rRodgau\r63110\rWiesenstr. 11\rWerkstr. 1\r5\r\r\r9\rRudi Biedritzky\rZaisentalstr. 70/1\rReutlingen\r72760\rZaisentalstr. 70/1\rHauptstr. 10B\r10\r\r\r\rdata_fin = pd.merge(left=data_both, right=data2, how=\u0026#34;left\u0026#34;, left_on=\u0026#34;address_2\u0026#34;, right_on=\u0026#34;Address\u0026#34;) \r\rCompany.Name\rCity\rPostcode\raddress_1\raddress_2\rdist\rPostal.Code\rLocation\r\r\r\r\r0\rMario Tsiknas\rBad Zwischenahn\r26160\rAm Delf 31\rWerkstr. 1\r9\r24837\rSchleswig\r\r\r1\rAndre Hanisch\rUntereisesheim\r74257\rIm Kressgraben 18\rAm neuen Markt 8\r12\r66877\rRamstein-Miesenbach\r\r\r2\rMatthias Essers GmbH Elektrote\rGeilenkirchen\r52511\rLeopold-Hoesch-Str. 4\rEdisonstr. 36\r15\r04435\rSchkeuditz\r\r\r3\rGerold Fuchs\rDietingen\r78661\rMühlweg 12\rWerkstr. 1\r9\r24837\rSchleswig\r\r\r4\rZirpel \u0026amp; Pautzsch Ingenieur Pa\rAltenburg\r4600\rPaditzer Str. 33\rEdisonstr. 36\r9\r04435\rSchkeuditz\r\r\r5\rEberhard Zessin\rHeidelberg\r69118\rSteingartenweg 12\rWerkstr. 1\r13\r24837\rSchleswig\r\r\r6\rPaul Strigl\rDachau\r85221\rThomas-Schwarz-Str. 26\rEdisonstr. 36\r16\r04435\rSchkeuditz\r\r\r7\rCarsten Helm\rWismar\r23966\rUlmenstr. 8\rWerkstr. 1\r6\r24837\rSchleswig\r\r\r8\rWolfgang Jäger\rRodgau\r63110\rWiesenstr. 11\rWerkstr. 1\r5\r24837\rSchleswig\r\r\r9\rRudi Biedritzky\rReutlingen\r72760\rZaisentalstr. 70/1\rHauptstr. 10B\r10\r66459\rKirkel\r\r\r\rMultiprocessing Because these operations can be computationally expensive, we can use multiprocessing to hasten its execution.\nfrom joblib import Parallel, delayed def dl_dist_func(arr1, arr2): dl_dist = {} for s1 in arr1: for s2 in arr2: temp_dist = damerau_levenshtein(s1, s2) dl_dist[f\u0026#34;{s1}_and_ {s2}\u0026#34;] = temp_dist return dl_dist dl_dist_par = Parallel()( delayed(dl_dist_func)(arr1, arr2) for arr1, arr2 in zip([data1[\u0026#34;Address\u0026#34;]], [data2[\u0026#34;Address\u0026#34;]]) ) list(dl_dist_par.pop().items())[:10] [('Ulmenstr. 8 _and_ Alfons-Müller-Platz ', 16),\r('Ulmenstr. 8 _and_ Hauptstr. 10B', 8),\r('Ulmenstr. 8 _and_ Edisonstr. 36', 7),\r('Ulmenstr. 8 _and_ Kutterstr. 3 / 26386 Wilhelmshaven-Rüstersiel', 38),\r('Ulmenstr. 8 _and_ Am neuen Markt 8', 11),\r('Ulmenstr. 8 _and_ Rübenkamp 226', 10),\r('Ulmenstr. 8 _and_ Geisenheimer Str. 10', 14),\r('Ulmenstr. 8 _and_ Rathausplatz 1', 12),\r('Ulmenstr. 8 _and_ Weilimdorfer Str. 74 2', 15),\r('Ulmenstr. 8 _and_ Werkstr. 1', 6)]\r ","permalink":"https://data-simplified.dev/posts/2022-01-12/fuzzy_join/","summary":"Using fuzzy matching to join tables.","title":"Fuzzy Joins and Multiprocessing"},{"content":"I’m a data analytics specialist with interests in: data science, machine learning, deep learning, statistics, business intelligence, analytics, data engineering, MLOps, software engineering, data governance, and data strategy.\nI have a little over 10 years' experience (as at 2021) and over the time, I\u0026rsquo;ve had the good fortune of doing a very wide variety of projects: started as a consultant with one of the Big Four firms doing technology audits, transitioned to technology advisory focusing on tech strategy development, system implementations, and data analytics, moved to a tier 1 bank doing data governance, moved back to consulting and did a lot of ML and BI projects, then joined a start-up focusing on application of ML and statistics in Behavioural Science.\nI\u0026rsquo;ve worked in all sorts of industries and with all sorts of internal teams - something that I\u0026rsquo;ve come to value over time as I tend to have a better understanding of the perspectives and needs of my colleagues from other functions.\nI\u0026rsquo;m also an entrepreneur in the food, agriculture, and tech spaces ;-)\nQuite the passionate learner - my mantra is NO MORE ZERO DAYS!! It has a great story on Reddit by the way. You should check it out.\n","permalink":"https://data-simplified.dev/aboutme/","summary":"I’m a data analytics specialist with interests in: data science, machine learning, deep learning, statistics, business intelligence, analytics, data engineering, MLOps, software engineering, data governance, and data strategy.\nI have a little over 10 years' experience (as at 2021) and over the time, I\u0026rsquo;ve had the good fortune of doing a very wide variety of projects: started as a consultant with one of the Big Four firms doing technology audits, transitioned to technology advisory focusing on tech strategy development, system implementations, and data analytics, moved to a tier 1 bank doing data governance, moved back to consulting and did a lot of ML and BI projects, then joined a start-up focusing on application of ML and statistics in Behavioural Science.","title":"About Me"},{"content":"We want to test the scaling and unscaling of values.\nlibrary(tidyverse) data_raw \u0026lt;- read_csv(file = \u0026#34;Train.csv\u0026#34;) %\u0026gt;% select(dispatch_day, dispatch_day, order_carrier_type, rider_license_status, rider_amount) %\u0026gt;% head(20) data_raw ## # A tibble: 20 x 4\r## dispatch_day order_carrier_type rider_license_status rider_amount\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 27 2 0 1080\r## 2 30 1 0 730\r## 3 14 2 1 490\r## 4 22 2 1 510\r## 5 27 2 0 400\r## 6 22 1 1 400\r## 7 14 1 1 400\r## 8 3 1 0 420\r## 9 18 2 0 420\r## 10 28 2 0 800\r## 11 20 1 0 450\r## 12 28 2 1 400\r## 13 6 2 1 420\r## 14 28 1 0 610\r## 15 12 1 0 450\r## 16 17 1 1 400\r## 17 7 1 1 410\r## 18 20 2 0 400\r## 19 5 1 0 400\r## 20 1 2 0 390\r Scenario 1: Vector or Single Column Note the attributes of this matrix (center, scale) which can be used to unscale the values. When converted to a tibble, these attributes are lost.\ndata_scaled_r \u0026lt;- scale(x = data_raw$rider_amount, center = TRUE, scale = TRUE) data_scaled_r ## [,1]\r## [1,] 3.25329756\r## [2,] 1.29347975\r## [3,] -0.05039532\r## [4,] 0.06159427\r## [5,] -0.55434847\r## [6,] -0.55434847\r## [7,] -0.55434847\r## [8,] -0.44235888\r## [9,] -0.44235888\r## [10,] 1.68544331\r## [11,] -0.27437449\r## [12,] -0.55434847\r## [13,] -0.44235888\r## [14,] 0.62154222\r## [15,] -0.27437449\r## [16,] -0.55434847\r## [17,] -0.49835367\r## [18,] -0.55434847\r## [19,] -0.55434847\r## [20,] -0.61034326\r## attr(,\u0026quot;scaled:center\u0026quot;)\r## [1] 499\r## attr(,\u0026quot;scaled:scale\u0026quot;)\r## [1] 178.588\r These are the values of the scaling and centering used.\nattr(x = data_scaled_r, which = \u0026#34;scaled:center\u0026#34;) ## [1] 499\r attr(x = data_scaled_r, which = \u0026#34;scaled:scale\u0026#34;) ## [1] 178.588\r data_scaled_r * attr(x = data_scaled_r, which = \u0026#34;scaled:scale\u0026#34;) + attr(x = data_scaled_r, which = \u0026#34;scaled:center\u0026#34;) ## [,1]\r## [1,] 1080\r## [2,] 730\r## [3,] 490\r## [4,] 510\r## [5,] 400\r## [6,] 400\r## [7,] 400\r## [8,] 420\r## [9,] 420\r## [10,] 800\r## [11,] 450\r## [12,] 400\r## [13,] 420\r## [14,] 610\r## [15,] 450\r## [16,] 400\r## [17,] 410\r## [18,] 400\r## [19,] 400\r## [20,] 390\r## attr(,\u0026quot;scaled:center\u0026quot;)\r## [1] 499\r## attr(,\u0026quot;scaled:scale\u0026quot;)\r## [1] 178.588\r Scenario 2: Many Columns data_scaled \u0026lt;- scale(x = data_raw, center = TRUE, scale = TRUE) data_scaled ## dispatch_day order_carrier_type rider_license_status rider_amount\r## [1,] 1.02264634 0.9746794 -0.7958224 3.25329756\r## [2,] 1.34389651 -0.9746794 -0.7958224 1.29347975\r## [3,] -0.36943768 0.9746794 1.1937336 -0.05039532\r## [4,] 0.48722941 0.9746794 1.1937336 0.06159427\r## [5,] 1.02264634 0.9746794 -0.7958224 -0.55434847\r## [6,] 0.48722941 -0.9746794 1.1937336 -0.55434847\r## [7,] -0.36943768 -0.9746794 1.1937336 -0.55434847\r## [8,] -1.54735494 -0.9746794 -0.7958224 -0.44235888\r## [9,] 0.05889586 0.9746794 -0.7958224 -0.44235888\r## [10,] 1.12972973 0.9746794 -0.7958224 1.68544331\r## [11,] 0.27306264 -0.9746794 -0.7958224 -0.27437449\r## [12,] 1.12972973 0.9746794 1.1937336 -0.55434847\r## [13,] -1.22610478 0.9746794 1.1937336 -0.44235888\r## [14,] 1.12972973 -0.9746794 -0.7958224 0.62154222\r## [15,] -0.58360446 -0.9746794 -0.7958224 -0.27437449\r## [16,] -0.04818752 -0.9746794 1.1937336 -0.55434847\r## [17,] -1.11902139 -0.9746794 1.1937336 -0.49835367\r## [18,] 0.27306264 0.9746794 -0.7958224 -0.55434847\r## [19,] -1.33318817 -0.9746794 -0.7958224 -0.55434847\r## [20,] -1.76152171 0.9746794 -0.7958224 -0.61034326\r## attr(,\u0026quot;scaled:center\u0026quot;)\r## dispatch_day order_carrier_type rider_license_status ## 17.45 1.50 0.40 ## rider_amount ## 499.00 ## attr(,\u0026quot;scaled:scale\u0026quot;)\r## dispatch_day order_carrier_type rider_license_status ## 9.3385167 0.5129892 0.5026247 ## rider_amount ## 178.5880293\r scale \u0026lt;- attr(data_scaled, which = \u0026#34;scaled:scale\u0026#34;) center \u0026lt;- attr(data_scaled, which = \u0026#34;scaled:center\u0026#34;) apply(X = data_scaled, MARGIN = 1, FUN = function(x) x * scale + center) %\u0026gt;% t() %\u0026gt;% as_tibble() ## # A tibble: 20 x 4\r## dispatch_day order_carrier_type rider_license_status rider_amount\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 27 2 0 1080\r## 2 30 1 0 730\r## 3 14 2 1 490\r## 4 22 2 1 510\r## 5 27 2 0 400\r## 6 22 1 1 400\r## 7 14 1 1 400\r## 8 3 1 0 420\r## 9 18 2 0 420\r## 10 28 2 0 800\r## 11 20 1 0 450\r## 12 28 2 1 400\r## 13 6 2 1 420\r## 14 28 1 0 610\r## 15 12 1 0 450\r## 16 17 1 1 400\r## 17 7.00 1 1 410\r## 18 20 2 0 400\r## 19 5 1 0 400\r## 20 1 2 0 390\r ","permalink":"https://data-simplified.dev/posts/scale-unscale/","summary":"A demo of how to scale then unscale data when training an ML model.","title":"Scaling and Unscaling Data"}]